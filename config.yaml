data:
  tokenizer: /Users/19891176/Desktop/MyProjects/CLIP/.exp/dump/vocab.txt
  max_length: 240
  image:
    dataframe: /Users/19891176/Desktop/MyProjects/CLIP/.exp/data.csv
    batch_sizes:
      train: 16
      valid: 16
      test: 16
    split_sizes:
      train: 0.4
      valid: 0.4
      test: 0.2
    num_workers: 0
    image_column: image_path
    text_column: ai_description
  text:
    dataframe: /Users/19891176/Desktop/MyProjects/CLIP/.exp/texts.tsv
    batch_sizes:
      train: 16
      valid: 16
      test: 16
    split_sizes:
      train: 0.4
      valid: 0.4
      test: 0.2
    num_workers: 0
    text_column: text
  clip:
    dataframe: /Users/19891176/Desktop/MyProjects/CLIP/.exp/data.csv
    batch_sizes:
      train: 16
      valid: 16
      test: 16
    split_sizes:
      train: 0.4
      valid: 0.4
      test: 0.2
    num_workers: 0
    image_column: image_path
    text_column: ai_description
model:
  image:
    model_type: convnext
    drop_path_rate: 0.3
    depths: [3, 3, 3, 3]
    hidden_sizes: [16, 32, 64, 128]
    count_classes: 1000
  text:
    model_type: distilbert
    vocab_size: 8000
    hidden_size: 128
    num_hidden_layers: 2
    num_attention_heads: 2
    intermediate_size: 512
    hidden_dropout_prob: 0.3
    attention_probs_dropout_prob: 0.3
    max_position_embeddings: 240
    position_embedding_type: relative_key_query
